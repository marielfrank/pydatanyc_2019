{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex for noise removal\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk for some preprocessing wonders\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mariel/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/mariel/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/mariel/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download nltk corpora\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passage from Through the Looking Glass\n",
    "text = \"The shop seemed to be full of all manner of curious things â€” but the oddest part of it all was, that whenever she looked hard at any shelf, to make out exactly what it had on it, that particular shelf was always quite empty: though the others round it were crowded as full as they could hold.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shop seemed to be full of all manner of curious things but the oddest part of it all was that whenever she looked hard at any shelf to make out exactly what it had on it that particular shelf was always quite empty though the others round it were crowded as full as they could hold \n"
     ]
    }
   ],
   "source": [
    "# noise removal using regex\n",
    "cleaned = re.sub('\\W+', ' ', text)\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'shop', 'seemed', 'to', 'be', 'full', 'of', 'all', 'manner', 'of', 'curious', 'things', 'but', 'the', 'oddest', 'part', 'of', 'it', 'all', 'was', 'that', 'whenever', 'she', 'looked', 'hard', 'at', 'any', 'shelf', 'to', 'make', 'out', 'exactly', 'what', 'it', 'had', 'on', 'it', 'that', 'particular', 'shelf', 'was', 'always', 'quite', 'empty', 'though', 'the', 'others', 'round', 'it', 'were', 'crowded', 'as', 'full', 'as', 'they', 'could', 'hold']\n"
     ]
    }
   ],
   "source": [
    "# tokenization using word_tokenize() from nltk\n",
    "tokenized = word_tokenize(cleaned)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text:\n",
      " ['the', 'shop', 'seem', 'to', 'be', 'full', 'of', 'all', 'manner', 'of', 'curiou', 'thing', 'but', 'the', 'oddest', 'part', 'of', 'it', 'all', 'wa', 'that', 'whenev', 'she', 'look', 'hard', 'at', 'ani', 'shelf', 'to', 'make', 'out', 'exactli', 'what', 'it', 'had', 'on', 'it', 'that', 'particular', 'shelf', 'wa', 'alway', 'quit', 'empti', 'though', 'the', 'other', 'round', 'it', 'were', 'crowd', 'as', 'full', 'as', 'they', 'could', 'hold']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "print(\"Stemmed text:\\n\", stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized text:\n",
      " ['The', 'shop', 'seemed', 'to', 'be', 'full', 'of', 'all', 'manner', 'of', 'curious', 'thing', 'but', 'the', 'oddest', 'part', 'of', 'it', 'all', 'wa', 'that', 'whenever', 'she', 'looked', 'hard', 'at', 'any', 'shelf', 'to', 'make', 'out', 'exactly', 'what', 'it', 'had', 'on', 'it', 'that', 'particular', 'shelf', 'wa', 'always', 'quite', 'empty', 'though', 'the', 'others', 'round', 'it', 'were', 'crowded', 'a', 'full', 'a', 'they', 'could', 'hold']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    "print(\"Lemmatized text:\\n\", lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# luckily we've prepared for this...\n",
    "from part_of_speech import get_part_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text lemmatized with part of speech:\n",
      " ['The', 'shop', 'seem', 'to', 'be', 'full', 'of', 'all', 'manner', 'of', 'curious', 'thing', 'but', 'the', 'odd', 'part', 'of', 'it', 'all', 'be', 'that', 'whenever', 'she', 'look', 'hard', 'at', 'any', 'shelf', 'to', 'make', 'out', 'exactly', 'what', 'it', 'have', 'on', 'it', 'that', 'particular', 'shelf', 'be', 'always', 'quite', 'empty', 'though', 'the', 'others', 'round', 'it', 'be', 'crowd', 'a', 'full', 'a', 'they', 'could', 'hold']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_with_pos = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "print(\"Text lemmatized with part of speech:\\n\", lemmatized_with_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# what if we removed unnecessary common words?\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(\"Stopwords:\", stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Filtered for stopwords:\n",
      " ['shop', 'seemed', 'full', 'manner', 'curious', 'things', 'oddest', 'part', 'whenever', 'looked', 'hard', 'shelf', 'make', 'exactly', 'particular', 'shelf', 'always', 'quite', 'empty', 'though', 'others', 'round', 'crowded', 'full', 'could', 'hold']\n"
     ]
    }
   ],
   "source": [
    "filtered = [word for word in tokenized if word.lower() not in stop_words]\n",
    "print(\"Text Filtered for stopwords:\\n\", filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
